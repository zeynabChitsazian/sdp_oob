{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynabChitsazian/sdp_oob/blob/main/sdp_oob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcBoMLzANcg7"
      },
      "source": [
        "This code is to identify the defective commit of individual projects using online adaptive GMM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "Z4thluJY4n5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmLWdg7TULTB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from joblib import dump\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "from itertools import combinations\n",
        "from sklearn.decomposition import PCA\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwAU76C1xfCQ"
      },
      "outputs": [],
      "source": [
        "#merging the datasets as daily\n",
        "from datetime import datetime\n",
        "def merging_datasets(outputPath, inputPath, dfs_path):\n",
        "  contents = []\n",
        "  for filename in os.listdir(inputPath):\n",
        "    print(filename)\n",
        "    content = pd.read_csv(inputPath + filename)\n",
        "    contents.append(content) #[::-1] and reset index\n",
        "\n",
        "  combined_data = pd.concat(contents)\n",
        "\n",
        "  def extract_month_year(row):\n",
        "    date = datetime.utcfromtimestamp(row['author_date']).date()\n",
        "    year, month, dnum = date.strftime(\"%Y-%m-%d\").split(\"-\")\n",
        "    return pd.Series({'dnum': dnum, 'month': month, 'year': year})\n",
        "\n",
        "  # Apply the function to all rows of the data frame\n",
        "  combined_data[['dnum', 'month', 'year']] = combined_data.apply(lambda row: extract_month_year(row), axis=1)\n",
        "  combined_data['year'] = combined_data['year'].astype(int)\n",
        "  combined_data['month'] = combined_data['month'].astype(int)\n",
        "  combined_data['dnum'] = combined_data['dnum'].astype(int)\n",
        "\n",
        "  #Each cell in 'dfs' include commits of 1 day\n",
        "  dfs = [] #merged datasets\n",
        "\n",
        "  for year in range(2003 , 2020):\n",
        "    print('year: ', year)\n",
        "    for month in range(1, 13):\n",
        "      for dnum in range(1, 32):\n",
        "        new_df = combined_data[(combined_data['dnum'] == dnum) & (combined_data['month'] == month) & (combined_data['year'] == year)]\n",
        "        dfs.append(new_df)\n",
        "\n",
        "  if not os.path.exists(outputPath):\n",
        "      os.makedirs(outputPath)\n",
        "      print(f\"Folder '{outputPath}' created successfully.\")\n",
        "  # Save the array dfs to a file\n",
        "  with open(dfs_path, 'wb') as file:\n",
        "      pickle.dump(dfs, file)\n",
        "\n",
        "  print('dfs size: ', len(dfs))\n",
        "  return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhIi5Vt_xlRp"
      },
      "outputs": [],
      "source": [
        "#separating a dataset as daily\n",
        "def separating_daily(outputPath, inputPath, target_fileName, dfTest_path):\n",
        "\n",
        "    content_test = pd.read_csv(inputPath + target_fileName + '.csv')\n",
        "\n",
        "    def extract_month_year(row):\n",
        "      date = datetime.utcfromtimestamp(row['author_date']).date()\n",
        "      year, month, dnum = date.strftime(\"%Y-%m-%d\").split(\"-\")\n",
        "      return pd.Series({'dnum': dnum, 'month': month, 'year': year})\n",
        "\n",
        "    content_test[['dnum','month', 'year']] = content_test.apply(lambda row: extract_month_year(row), axis=1)\n",
        "    content_test['year'] = content_test['year'].astype(int)\n",
        "    content_test['month'] = content_test['month'].astype(int)\n",
        "    content_test['dnum'] = content_test['dnum'].astype(int)\n",
        "\n",
        "    #Each cell in 'df_test' include commits of 1 day\n",
        "    df_test = []\n",
        "\n",
        "    for year in range(2003 , 2020):\n",
        "      print('year: ', year)\n",
        "      for month in range(1, 13):\n",
        "        for dnum in range(1, 32):\n",
        "          new_dfTest = content_test[(content_test['dnum'] == dnum) & (content_test['month'] == month) & (content_test['year'] == year)]\n",
        "          df_test.append(new_dfTest)\n",
        "\n",
        "    folderName = outputPath + target_fileName\n",
        "    if not os.path.exists(folderName):\n",
        "      os.makedirs(folderName)\n",
        "      print(f\"Folder '{folderName}' created successfully.\")\n",
        "\n",
        "    with open(dfTest_path, 'wb') as file:\n",
        "        pickle.dump(df_test, file)\n",
        "\n",
        "    print('df_test size: ', len(df_test))\n",
        "    return df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzUGnKxsURFA"
      },
      "outputs": [],
      "source": [
        "#pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bz08tWWTXPR"
      },
      "outputs": [],
      "source": [
        "#pip install scikit-multiflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "DqeYaPJjvMp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7244259e-c069-42cf-a8de-9730280f035c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "!conda install -c anaconda scikit-multiflow"
      ],
      "metadata": {
        "id": "nNwOe2vNvTKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW0_ZW1PSeFn"
      },
      "outputs": [],
      "source": [
        "from skmultiflow.meta import OzaBagging\n",
        "from skmultiflow.trees import HoeffdingTree\n",
        "def create_initial_model(data):\n",
        "  y_train = data['contains_bug']\n",
        "  X_train = data.drop('contains_bug', axis=1)\n",
        "  X_train_array = X_train.to_numpy()\n",
        "  y_train_array = y_train.to_numpy()\n",
        "  classes = np.unique(y_train_array)\n",
        "  oza_bagging = OzaBagging(base_estimator=HoeffdingTree(), n_estimators=20)\n",
        "  oza_bagging.partial_fit(X_train_array, y_train_array, classes=classes)\n",
        "\n",
        "  return oza_bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCd-Lcje1TIn"
      },
      "outputs": [],
      "source": [
        "#In the code below, we have obtained recall for defect and clean using equation\n",
        "#in sadia paper which is based on fading factor\n",
        "def update_model(model, data):\n",
        "  teta=0.99\n",
        "  correct_labels = data['contains_bug']\n",
        "  dataDropLabels = data.drop('contains_bug', axis=1)\n",
        "  X_train = dataDropLabels.to_numpy()\n",
        "  y_train = correct_labels.to_numpy()\n",
        "  predictions = model.predict(X_train)\n",
        "  for i in range(len(data)):\n",
        "    correct_label = int(correct_labels[i])\n",
        "    imRate[correct_label] = round(teta * imRate[correct_label] + (1-teta)*int(predictions[i] == correct_label),2)\n",
        "    lamb = 1\n",
        "    if correct_label == 1 and imRate[1] < imRate[0]:\n",
        "      lamb = imRate[0]/imRate[1]\n",
        "    elif correct_label == 0 and imRate[0] < imRate[1]:\n",
        "      lamb = imRate[1]/imRate[0]\n",
        "\n",
        "    k = np.random.poisson(lamb)\n",
        "    X = np.tile(np.array(X_train[i]), (k, 1))\n",
        "    Y = np.full(k, y_train[i])\n",
        "    model.partial_fit(X, Y)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_s5hOaERmE"
      },
      "outputs": [],
      "source": [
        "def initial_evaluation(testData):\n",
        "  global recallNum\n",
        "  teta=0.99\n",
        "  correct_labels = testData['contains_bug'].to_numpy()\n",
        "  testDataDropLabels = testData.drop('contains_bug', axis=1).to_numpy()\n",
        "  prediction = 0\n",
        "  for i in range(len(testDataDropLabels)):\n",
        "    recall = []\n",
        "    recall.append(recalls[0][-1])\n",
        "    recall.append(recalls[1][-1])\n",
        "    correct_label = int(correct_labels[i])\n",
        "    R[correct_label] = teta * R[correct_label] + int(prediction == correct_label)\n",
        "    N[correct_label] = teta * N[correct_label] + 1\n",
        "    recall_value = round((R[correct_label]/N[correct_label])*100,2)\n",
        "    recalls[correct_label].append(recall_value)\n",
        "    temp[correct_label].append(recall_value)\n",
        "\n",
        "    recalls[1-correct_label].append(recall[1-correct_label])\n",
        "    temp[1-correct_label].append(recall[1-correct_label])\n",
        "    recallNum += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MAscgXPbxHk"
      },
      "outputs": [],
      "source": [
        "#In the code below, we have obtained recall for defect and clean using equation\n",
        "#in sadia paper which is based on fading factor\n",
        "def evaluation(online_bagging_classifier, testData):\n",
        "  global recallNum\n",
        "  teta=0.99\n",
        "  correct_labels = testData['contains_bug'].to_numpy()\n",
        "  testDataDropLabels = testData.drop('contains_bug', axis=1).to_numpy()\n",
        "  predictions = online_bagging_classifier.predict(testDataDropLabels)\n",
        "\n",
        "  for i in range(len(testDataDropLabels)):\n",
        "    recall = []\n",
        "    recall.append(recalls[0][-1])\n",
        "    recall.append(recalls[1][-1])\n",
        "\n",
        "    correct_label = int(correct_labels[i])\n",
        "    R[correct_label] = teta * R[correct_label] + int(predictions[i] == correct_label)\n",
        "    N[correct_label] = teta * N[correct_label] + 1\n",
        "    recall_value = round((R[correct_label]/N[correct_label])*100,2)\n",
        "    recalls[correct_label].append(recall_value)\n",
        "    temp[correct_label].append(recall_value)\n",
        "\n",
        "    recalls[1-correct_label].append(recall[1-correct_label])\n",
        "    temp[1-correct_label].append(recall[1-correct_label])\n",
        "    recallNum += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7TFt_r1yDTj"
      },
      "outputs": [],
      "source": [
        "def data_separation (data):\n",
        "\n",
        "  clean_samples = data.drop(data[data['contains_bug'] == 1].index).drop('contains_bug', axis=1)\n",
        "  defect_samples = data.drop(data[data['contains_bug'] == 0].index).drop('contains_bug', axis=1)\n",
        "\n",
        "  return clean_samples,defect_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVQsIUzLj4XA"
      },
      "outputs": [],
      "source": [
        "def data_normalization(data,status):\n",
        "\n",
        "  if status == 1:\n",
        "    #entropy = entropy/log2(n)\n",
        "    data['entrophy'] = data.apply(lambda row: row['entrophy'] / np.log2(row['nf']) if (row['nf'] != 0 and row['nf'] != 1) else row['entrophy'], axis=1)\n",
        "    data['nuc'] = data.apply(lambda row: row['nuc'] / row['nf'] if row['nf'] != 0 else row['nuc'], axis=1)\n",
        "    data['lt'] = data.apply(lambda row: row['lt'] / row['nf'] if row['nf'] != 0 else row['lt'], axis=1)\n",
        "    data['lt'] = np.abs(data['lt'])\n",
        "    data['churn'] = data.apply(lambda row: (row['la'] + row['ld']) / row['lt'] if row['lt'] != 0 else row['la'] + row['ld'], axis=1)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46RrUYSMxLRm"
      },
      "outputs": [],
      "source": [
        "def remove_correlation_features (data, Remove_extra_correlations_status):\n",
        "\n",
        "  correlated_features = []\n",
        "  #Check the correlation between the columns as 2 to 2\n",
        "  if Remove_extra_correlations_status == 1:\n",
        "    columns = data.columns.tolist()\n",
        "    for col1, col2 in combinations(columns, 2):\n",
        "      correlation, p_value = spearmanr(data[col1], data[col2])\n",
        "      if correlation > 0.8 and p_value < 0.05:\n",
        "        if col1 not in correlated_features and col2 not in correlated_features:\n",
        "          correlated_features.append(col1)\n",
        "  independent_data = data.drop(correlated_features, axis=1)\n",
        "\n",
        "  return independent_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ojHUoVZawdo"
      },
      "outputs": [],
      "source": [
        "def log_transform (data, status):\n",
        "\n",
        "  if status == 1:\n",
        "    data = data.astype(float)\n",
        "    data = np.log2(data[data != 0])\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMwDumoPmG2k"
      },
      "outputs": [],
      "source": [
        "def data_scaling (data, status):\n",
        "\n",
        "  if status == 1:\n",
        "    scaler = StandardScaler()\n",
        "    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQKX79mr4_hF"
      },
      "outputs": [],
      "source": [
        "def PCA_decomposition(data, n_com, status):\n",
        "\n",
        "  if status == 1:\n",
        "    pca = PCA(n_components = n_com)\n",
        "    data_transformed = pca.fit_transform(data)\n",
        "    data_transformed = pd.DataFrame(data_transformed)\n",
        "    data_transformed.columns = data_transformed.columns.astype(str)\n",
        "    return data_transformed\n",
        "  else:\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMtXdxc6xZf8"
      },
      "outputs": [],
      "source": [
        "def drop_nf0(data, status):\n",
        "  if status == 1:\n",
        "    #To remove the rows in which the number of changed files is equal to 0. Because it demostrates that no changes have been applied to the commit\n",
        "    data = data[data['nf'] != 0].reset_index(drop=True)\n",
        "  else:\n",
        "    data = data.reset_index(drop=True)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mT2oAv7EaJT"
      },
      "source": [
        "In this part, each line shows a step of data processing. So you can optionally remove each line and evaluate the overall performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSwJeYBN7-1q"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing (data):\n",
        "\n",
        "  data = drop_nf0(data, 0)\n",
        "\n",
        "  #normalizing data except fix feature and label column ('contains_bug')\n",
        "  column_fix = data['fix']\n",
        "  column_contains_bug = data['contains_bug']\n",
        "  #column_classification = data['classification']\n",
        "  data = data.drop('fix', axis=1).drop('contains_bug', axis=1).drop('dnum', axis=1).drop('month', axis=1).drop('year', axis=1).drop('author_date', axis=1)#.drop('classification', axis=1)\n",
        "\n",
        "  data = data_normalization(data,1)\n",
        "  #print('normalized_data: ', data.head(1))\n",
        "\n",
        "  #1) nf, exp yes. etc no\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1).drop('nd', axis=1).drop('ns', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  #2) only nf, etc\n",
        "  #data = data.drop('exp', axis=1).drop('ndev', axis=1).drop('age', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  #3) nf only no etc\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1).drop('nd', axis=1).drop('ns', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "  #data = data.drop('exp', axis=1).drop('ndev', axis=1).drop('age', axis=1)\n",
        "\n",
        "  #4) no nf etc\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1)\n",
        "\n",
        "  #data = data.drop('nd', axis=1).drop('ns', axis=1)\n",
        "\n",
        "  #5) no exp etc\n",
        "  #data = data.drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  data = log_transform (data, 1)\n",
        "  #data = data_scaling(data, 1)\n",
        "  data['fix'] = column_fix\n",
        "  final_data = remove_correlation_features(data, 0)\n",
        "  #finishing preprocessing steps\n",
        "\n",
        "  final_data['contains_bug'] = column_contains_bug\n",
        "\n",
        "  return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srKHFsujIk8H"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log2\")\n",
        "#------------------------------------------------------------------------------- Settings\n",
        "wp = True\n",
        "if wp: pklFile_rec = 'oob_recalls_wp.pkl'\n",
        "else: pklFile_rec = 'oob_recalls_dfs.pkl'\n",
        "interpret_defectPredict = True\n",
        "#------------------------------------------------------------------------------- Import datasets\n",
        "input_dir = \"drive/MyDrive/part2/data/sadia_csv/\"\n",
        "output_dir = 'drive/MyDrive/part2/data/code_outputs/'\n",
        "dfs_path = 'drive/MyDrive/part2/data/dfs' + '.pkl'\n",
        "files = os.listdir(input_dir)\n",
        "for filename in files:\n",
        "  target_fileName = filename[:-4]\n",
        "  dfTest_path = output_dir + target_fileName + '/df_test.pkl'\n",
        "  if os.path.isfile(dfTest_path):\n",
        "    with open(dfTest_path, 'rb') as file:\n",
        "        df_test = pickle.load(file)\n",
        "        print('There is the df_test file for ' + target_fileName + ' project.')\n",
        "  else: df_test = separating_daily(output_dir, input_dir, target_fileName, dfTest_path)\n",
        "\n",
        "  if wp:\n",
        "    dfs = df_test\n",
        "  else:\n",
        "    if os.path.isfile(dfs_path):\n",
        "      # Load the array dfs from the saved file\n",
        "      with open(dfs_path , 'rb') as file:\n",
        "        dfs = pickle.load(file)\n",
        "        print('There is the dfs' + ' file.')\n",
        "    else: dfs = merging_datasets(output_dir, input_dir, dfs_path)\n",
        "\n",
        "#------------------------------------------------------------------------------- Main code\n",
        "  start_time = time.time()\n",
        "  warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "  warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log2\")\n",
        "  all_start_time = time.time()\n",
        "#------------------------------------------------------------------------------- Adjust the settings\n",
        "  UpMflag = True #Update model flag\n",
        "  trainData_len = 1\n",
        "  Initial_size = 1 #The minimum number of samples required to build the model\n",
        "  vl = 90 #verification latency\n",
        "  startNum = 0\n",
        "  endNum = 5\n",
        "  rowNum = 0\n",
        "  defect_accsMean = 0\n",
        "  clean_accsMean = 0\n",
        "  flag = False\n",
        "  initData = pd.DataFrame()\n",
        "  testNum = 0\n",
        "  trainNum = 0\n",
        "#------------------------------------------------------------------------------- Global variables\n",
        "  global imRate\n",
        "  imRate = [1, 1]\n",
        "  global first_imRate\n",
        "  first_imRate = [True, True]\n",
        "  global num_init_sam_rate\n",
        "  num_init_sam_rate = [0, 0]\n",
        "  #If the new sample is a member of class C, the recall of the other class remains the same as the previous recall\n",
        "  global recalls\n",
        "  recalls = [[0],[0]]\n",
        "  global recallNum\n",
        "  recallNum = 0\n",
        "  global temp\n",
        "  temp = [[0],[0]]\n",
        "  global N\n",
        "  N = [0, 0]\n",
        "  global R\n",
        "  R = [0, 0]\n",
        "  global iN\n",
        "  iN = [1, 1]\n",
        "  global iR\n",
        "  iR = [1, 1]\n",
        "  def updatemodel (UpMflag, trainData, i):\n",
        "    if UpMflag: return pd.concat([trainData, dfs[i]], ignore_index=True)\n",
        "    else: return trainData\n",
        "#------------------------------------------------------------------------------ Creating training DS\n",
        "\n",
        "  while True:\n",
        "    #this shows, wether there is any elements in this section (for larger speed)\n",
        "    if len(pd.concat(dfs[startNum:endNum], ignore_index=True)) > 0:\n",
        "      for rowNum in range(startNum,endNum):\n",
        "        initData = pd.concat([initData, dfs[rowNum]], ignore_index=True)\n",
        "        if len(initData) > Initial_size:\n",
        "          preprocessed_data = data_preprocessing(initData)\n",
        "          normalized_clean_samples, normalized_defect_samples = data_separation(preprocessed_data)\n",
        "          if len(normalized_defect_samples) > 1 and len(normalized_clean_samples) > 1:\n",
        "            trainNum = len(preprocessed_data)\n",
        "            print('num clean: ', len(normalized_clean_samples))\n",
        "            print('num defect: ', len(normalized_defect_samples))\n",
        "            flag = True\n",
        "            break\n",
        "    if flag:\n",
        "      break\n",
        "\n",
        "    startNum = endNum\n",
        "    endNum += 5\n",
        "  print(len(initData), ' - ', rowNum, ' - ', trainNum)\n",
        "\n",
        "  #----------------------------------------------------------------------------- Initial modeling\n",
        "  testCount = rowNum+vl\n",
        "  trainData = pd.DataFrame()\n",
        "  testData = pd.concat(df_test[:testCount+1], ignore_index=True)\n",
        "  print('len testData:',len(testData))\n",
        "  if len(testData) > 1:\n",
        "    gmeans_temp = []\n",
        "    print('len testData:',len(testData))\n",
        "    testNum = len(testData)\n",
        "    initial_evaluation(testData)\n",
        "    recalls[0].pop(0)\n",
        "    recalls[1].pop(0)\n",
        "    if len(recalls[1])>0:    defect_accsMean = sum(recalls[1]) / len(recalls[1])\n",
        "    if len(recalls[0])>0:  clean_accsMean = sum(recalls[0]) / len(recalls[0])\n",
        "    print(\"idefect_accsMean: \", defect_accsMean, \" - defect_accs: \", recalls[1])\n",
        "    print(\"iclean_accsMean:\", clean_accsMean, \" - clean_accs:\", recalls[0])\n",
        "    if N[0]>0 and N[1]>0:\n",
        "      for j in range(len(temp[0])):\n",
        "        gmeans_temp.append(math.sqrt(temp[0][j] * temp[1][j]))\n",
        "    print('N: ', N)\n",
        "\n",
        "\n",
        "  init_recalls0_len = len(recalls[0])\n",
        "  init_recalls1_len = len(recalls[1])\n",
        "  temp[0] = []\n",
        "  temp[1] = []\n",
        "\n",
        "  print('init testCount: ', testCount)\n",
        "  print('init train num: ', trainNum)\n",
        "  print('init test num: ', testNum)\n",
        "  print('init recall num: ', recallNum)\n",
        "  #----------------------------------------------------------------------------- Training model\n",
        "  online_bagging_classifier = create_initial_model(preprocessed_data)\n",
        "  if interpret_defectPredict: explainer = shap.KernelExplainer(online_bagging_classifier.predict, normalized_defect_samples)\n",
        "  else: explainer = shap.KernelExplainer(online_bagging_classifier.predict, normalized_clean_samples)\n",
        "\n",
        "  times = []\n",
        "  trCln = 0\n",
        "  defect_ratio = 0\n",
        "  predictions = []\n",
        "  clnNum = 0\n",
        "  defNum = 0\n",
        "  #----------------------------------------------------------------------------- Interpretation variables\n",
        "  best_feature = 'best_feature'\n",
        "  last_NS = pd.DataFrame() #last normalized_samples\n",
        "  counterCD = 0\n",
        "  cd_points = []\n",
        "  df_shap = pd.DataFrame(columns=normalized_clean_samples.columns)\n",
        "  ind_shap = 0\n",
        "  #-----------------------------------------------------------------------------\n",
        "  for i in range(rowNum+1, len(df_test) - 1 - vl):\n",
        "    if i%100 == 0:\n",
        "      all_time = int(time.time()- start_time)\n",
        "      gmeans_temp = []\n",
        "      print(f\"Execution time: {all_time} seconds\")\n",
        "      if len(temp[0])>0: times.append(all_time)\n",
        "      start_time = time.time()\n",
        "      print(i, ' ******************** - ', target_fileName, ': ', len(temp[0]), ' - ', len(temp[1]) )\n",
        "      if len(temp[1])>0:    defect_accsMean = sum(temp[1]) / len(temp[1])\n",
        "      if len(temp[0])>0:  clean_accsMean = sum(temp[0]) / len(temp[0])\n",
        "      print(\"defect_accsMean: \", defect_accsMean, \" - defect_accs: \", temp[1])\n",
        "      print(\"clean_accsMean:\", clean_accsMean, \" - clean_accs:\", temp[0])\n",
        "      temp[0] = []\n",
        "      temp[1] = []\n",
        "    trainData = updatemodel(UpMflag, trainData, i)\n",
        "    #---------------------------------------------------------------------------- Adapting model\n",
        "    if len(trainData) > trainData_len:\n",
        "      preprocessed_data = data_preprocessing(trainData)\n",
        "      normalized_clean_samples, normalized_defect_samples = data_separation(preprocessed_data)\n",
        "      if interpret_defectPredict: last_NS = pd.concat([last_NDS, normalized_defect_samples], ignore_index=True)\n",
        "      else: last_NS = pd.concat([last_NCS, normalized_clean_samples], ignore_index=True)\n",
        "      online_bagging_classifier = update_model(online_bagging_classifier, preprocessed_data)\n",
        "      trainData = pd.DataFrame()\n",
        "      trainNum += len(preprocessed_data)\n",
        "    #--------------------------------------------------------------------------- Testing model\n",
        "    testCount += 1\n",
        "    testData = df_test[testCount]\n",
        "    if len(testData) > 0:\n",
        "      preprocessed_data = data_preprocessing(testData)\n",
        "      evaluation(online_bagging_classifier, preprocessed_data)\n",
        "      testData = pd.DataFrame()\n",
        "      testNum += len(preprocessed_data)\n",
        "#------------------------------------------------------------------------------- Interpretation\n",
        "\n",
        "      X_test = preprocessed_data.drop('contains_bug', axis=1)\n",
        "      shap_values = explainer.shap_values(X_test)\n",
        "      feature_importance = np.round(np.mean(np.abs(shap_values), axis=0),2)\n",
        "      feature_indices = np.argsort(feature_importance)[::-1]\n",
        "      num_top_features = 2\n",
        "      if best_feature == 'best_feature':\n",
        "        best_feature = X_test.columns[feature_indices[0]]\n",
        "      for j in feature_indices[:num_top_features]:\n",
        "        print(f\"Feature {X_test.columns[j]}: Importance = {feature_importance[j]:.3f}\")\n",
        "      if not best_feature == X_test.columns[feature_indices[0]] and len(last_NS)>0:\n",
        "        counterCD += 1\n",
        "        explainer = shap.KernelExplainer(online_bagging_classifier.predict, last_NS)\n",
        "        print('************************* Explainer is updated')\n",
        "        print('len last_NS: ', len(last_NS))\n",
        "        cd_points.append(testNum)\n",
        "        best_feature = X_test.columns[feature_indices[0]]\n",
        "        last_NS = pd.DataFrame()\n",
        "      new_row_df = pd.DataFrame(shap_values, columns=df_shap.columns)\n",
        "      df_shap = pd.concat([df_shap, new_row_df], ignore_index=True)\n",
        "#------------------------------------------------------------------------------- Saving interpretation result files\n",
        "  print('counterCD for ', target_fileName, ': ', counterCD)\n",
        "  csvFolder_name = output_dir + target_fileName + '/shap_result/'\n",
        "  if not os.path.exists(pkFolder_name):\n",
        "      os.makedirs(pkFolder_name)\n",
        "      print(f\"Folder '{pkFolder_name}' created successfully.\")\n",
        "  df_shap.to_csv(csvFolder_name + 'shap_oob.csv', index=False)\n",
        "  with open(csvFolder_name + 'cdPoints_oob', 'wb') as file:\n",
        "    pickle.dump(cd_points, file)\n",
        "#-------------------------------------------------------------------------------\n",
        "  recalls[0].pop(0)\n",
        "  recalls[1].pop(0)\n",
        "  defect_accsMean = sum(recalls[1]) / len(recalls[1])\n",
        "  clean_accsMean = sum(recalls[0]) / len(recalls[0])\n",
        "  print(\"defect_accsMean: \", defect_accsMean, \" - defect_accs: \", recalls[1])\n",
        "  print(\"clean_accsMean:\", clean_accsMean, \" - clean_accs:\", recalls[0])\n",
        "  gmeans = []\n",
        "  for i in range(len(recalls[0])):\n",
        "    gmeans.append(math.sqrt(recalls[0][i] * recalls[1][i]))\n",
        "\n",
        "  pkFolder_name = output_dir + target_fileName + '/recallFiles/'\n",
        "  if not os.path.exists(pkFolder_name):\n",
        "      os.makedirs(pkFolder_name)\n",
        "      print(f\"Folder '{pkFolder_name}' created successfully.\")\n",
        "  with open(pkFolder_name + pklFile_rec, 'wb') as file:\n",
        "      pickle.dump(recalls, file)\n",
        "\n",
        "  gmeans_avg = sum(gmeans) / len(gmeans)\n",
        "  time_avg = sum(times)/len(times)\n",
        "  print(\"gmeans_avg:\", gmeans_avg)\n",
        "  print('target_fileName: ', target_fileName)\n",
        "  print('time_avg: ', math.floor(time_avg + 0.5), ' second')\n",
        "  print('remained testData: ', len(testData))\n",
        "  print('init_recalls0_len: ', init_recalls0_len)\n",
        "  print('init_recalls1_len: ', init_recalls1_len)\n",
        "  print('recalls0_len: ', len(recalls[0]))\n",
        "  print('recalls1_len: ', len(recalls[1]))\n",
        "  print('end train num: ', trainNum)\n",
        "  print('end test num: ', testNum)\n",
        "  print('end recall num: ', recallNum)\n",
        "  print('All Time = ', math.floor(int(time.time() - all_start_time)/60 + 0.5), ' minutes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63bE34gt-DWZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(recalls[1], 'ro', linestyle='dotted')\n",
        "#plt.plot(accs, 'bo', linestyle='dotted')\n",
        "plt.plot(recalls[0], 'go', linestyle='dotted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Line Diagram for Array')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQa7op9u4DGN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "arr = []\n",
        "arr.append(recalls[1])\n",
        "arr.append(recalls[0])\n",
        "arr.append(gmeans)\n",
        "plt.boxplot(arr)\n",
        "#plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJ6LcFczy7h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}