{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynabChitsazian/sdp_oob/blob/main/SDP_oob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcBoMLzANcg7"
      },
      "source": [
        "This code is to identify the defective commit of individual projects using online adaptive GMM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4thluJY4n5v",
        "outputId": "3a9fc47b-711d-4008-af1c-e7b2699039e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from shap) (1.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/site-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/site-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting numba (from shap)\n",
            "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting cloudpickle (from shap)\n",
            "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->shap)\n",
            "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
            "Successfully installed cloudpickle-3.1.0 llvmlite-0.43.0 numba-0.60.0 shap-0.46.0 slicer-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmLWdg7TULTB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from joblib import dump\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "from itertools import combinations\n",
        "from sklearn.decomposition import PCA\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwAU76C1xfCQ"
      },
      "outputs": [],
      "source": [
        "#merging the datasets as daily\n",
        "from datetime import datetime\n",
        "def merging_datasets(outputPath, inputPath, dfs_path):\n",
        "  contents = []\n",
        "  for filename in os.listdir(inputPath):\n",
        "    print(filename)\n",
        "    content = pd.read_csv(inputPath + filename)\n",
        "    contents.append(content) #[::-1] and reset index\n",
        "\n",
        "  combined_data = pd.concat(contents)\n",
        "\n",
        "  def extract_month_year(row):\n",
        "    date = datetime.utcfromtimestamp(row['author_date']).date()\n",
        "    year, month, dnum = date.strftime(\"%Y-%m-%d\").split(\"-\")\n",
        "    return pd.Series({'dnum': dnum, 'month': month, 'year': year})\n",
        "\n",
        "  # Apply the function to all rows of the data frame\n",
        "  combined_data[['dnum', 'month', 'year']] = combined_data.apply(lambda row: extract_month_year(row), axis=1)\n",
        "  combined_data['year'] = combined_data['year'].astype(int)\n",
        "  combined_data['month'] = combined_data['month'].astype(int)\n",
        "  combined_data['dnum'] = combined_data['dnum'].astype(int)\n",
        "\n",
        "  #Each cell in 'dfs' include commits of 1 day\n",
        "  dfs = [] #merged datasets\n",
        "\n",
        "  for year in range(2003 , 2020):\n",
        "    print('year: ', year)\n",
        "    for month in range(1, 13):\n",
        "      for dnum in range(1, 32):\n",
        "        new_df = combined_data[(combined_data['dnum'] == dnum) & (combined_data['month'] == month) & (combined_data['year'] == year)]\n",
        "        dfs.append(new_df)\n",
        "\n",
        "  if not os.path.exists(outputPath):\n",
        "      os.makedirs(outputPath)\n",
        "      print(f\"Folder '{outputPath}' created successfully.\")\n",
        "  # Save the array dfs to a file\n",
        "  with open(dfs_path, 'wb') as file:\n",
        "      pickle.dump(dfs, file)\n",
        "\n",
        "  print('dfs size: ', len(dfs))\n",
        "  return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhIi5Vt_xlRp"
      },
      "outputs": [],
      "source": [
        "#separating a dataset as daily\n",
        "def separating_daily(outputPath, inputPath, target_fileName, dfTest_path):\n",
        "\n",
        "    content_test = pd.read_csv(inputPath + target_fileName + '.csv')\n",
        "\n",
        "    def extract_month_year(row):\n",
        "      date = datetime.utcfromtimestamp(row['author_date']).date()\n",
        "      year, month, dnum = date.strftime(\"%Y-%m-%d\").split(\"-\")\n",
        "      return pd.Series({'dnum': dnum, 'month': month, 'year': year})\n",
        "\n",
        "    content_test[['dnum','month', 'year']] = content_test.apply(lambda row: extract_month_year(row), axis=1)\n",
        "    content_test['year'] = content_test['year'].astype(int)\n",
        "    content_test['month'] = content_test['month'].astype(int)\n",
        "    content_test['dnum'] = content_test['dnum'].astype(int)\n",
        "\n",
        "    #Each cell in 'df_test' include commits of 1 day\n",
        "    df_test = []\n",
        "\n",
        "    for year in range(2003 , 2020):\n",
        "      print('year: ', year)\n",
        "      for month in range(1, 13):\n",
        "        for dnum in range(1, 32):\n",
        "          new_dfTest = content_test[(content_test['dnum'] == dnum) & (content_test['month'] == month) & (content_test['year'] == year)]\n",
        "          df_test.append(new_dfTest)\n",
        "\n",
        "    folderName = outputPath + target_fileName\n",
        "    if not os.path.exists(folderName):\n",
        "      os.makedirs(folderName)\n",
        "      print(f\"Folder '{folderName}' created successfully.\")\n",
        "\n",
        "    with open(dfTest_path, 'wb') as file:\n",
        "        pickle.dump(df_test, file)\n",
        "\n",
        "    print('df_test size: ', len(df_test))\n",
        "    return df_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE6FpZPlPNLA",
        "outputId": "151bcdb8-f7a6-4ab3-ca47-07bba671284a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.10\n",
            "    /usr/local/lib/python3.10/site-packages/numpy-1.23.5.dist-info/*\n",
            "    /usr/local/lib/python3.10/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\n",
            "    /usr/local/lib/python3.10/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/site-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzUGnKxsURFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df36cfbe-ff66-4608-960f-ec1bbed4c01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        }
      ],
      "source": [
        "pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bz08tWWTXPR"
      },
      "outputs": [],
      "source": [
        "#pip install scikit-multiflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "DqeYaPJjvMp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bfa8cd-613d-411c-9748-3e615af111e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "!conda install -c anaconda scikit-multiflow"
      ],
      "metadata": {
        "id": "nNwOe2vNvTKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27512854-79cc-49ab-94e8-f9aee831f9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - anaconda\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c anaconda conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW0_ZW1PSeFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea18c117-09b7-46b6-f6e7-00c320a593a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5d3539d958ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOzaBagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHoeffdingTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_initial_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contains_bug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'contains_bug'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/skmultiflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrift_detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/skmultiflow/evaluation/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluate_prequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluatePrequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluate_prequential_delayed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluatePrequentialDelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluate_holdout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluateHoldout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/skmultiflow/evaluation/evaluate_prequential.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_evaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/skmultiflow/evaluation/base_evaluator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_data_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationDataBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_visualizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWindowClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mMultiLabelClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWindowMultiLabelClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRegressionMeasurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWindowRegressionMeasurements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/skmultiflow/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classification_performance_evaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classification_performance_evaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindowClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classification_performance_evaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelClassificationPerformanceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/skmultiflow/metrics/_classification_performance_evaluator.pyx\u001b[0m in \u001b[0;36minit skmultiflow.metrics._classification_performance_evaluator\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ],
      "source": [
        "from skmultiflow.meta import OzaBagging\n",
        "from skmultiflow.trees import HoeffdingTree\n",
        "def create_initial_model(data):\n",
        "  y_train = data['contains_bug']\n",
        "  X_train = data.drop('contains_bug', axis=1)\n",
        "  X_train_array = X_train.to_numpy()\n",
        "  y_train_array = y_train.to_numpy()\n",
        "  classes = np.unique(y_train_array)\n",
        "  oza_bagging = OzaBagging(base_estimator=HoeffdingTree(), n_estimators=20)\n",
        "  oza_bagging.partial_fit(X_train_array, y_train_array, classes=classes)\n",
        "\n",
        "  return oza_bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCd-Lcje1TIn"
      },
      "outputs": [],
      "source": [
        "#In the code below, we have obtained recall for defect and clean using equation\n",
        "#in sadia paper which is based on fading factor\n",
        "def update_model(model, data):\n",
        "  teta=0.99\n",
        "  correct_labels = data['contains_bug']\n",
        "  dataDropLabels = data.drop('contains_bug', axis=1)\n",
        "  X_train = dataDropLabels.to_numpy()\n",
        "  y_train = correct_labels.to_numpy()\n",
        "  predictions = model.predict(X_train)\n",
        "  for i in range(len(data)):\n",
        "    correct_label = int(correct_labels[i])\n",
        "    imRate[correct_label] = round(teta * imRate[correct_label] + (1-teta)*int(predictions[i] == correct_label),2)\n",
        "    lamb = 1\n",
        "    if correct_label == 1 and imRate[1] < imRate[0]:\n",
        "      lamb = imRate[0]/imRate[1]\n",
        "    elif correct_label == 0 and imRate[0] < imRate[1]:\n",
        "      lamb = imRate[1]/imRate[0]\n",
        "\n",
        "    k = np.random.poisson(lamb)\n",
        "    X = np.tile(np.array(X_train[i]), (k, 1))\n",
        "    Y = np.full(k, y_train[i])\n",
        "    model.partial_fit(X, Y)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_s5hOaERmE"
      },
      "outputs": [],
      "source": [
        "def initial_evaluation(testData):\n",
        "  global recallNum\n",
        "  teta=0.99\n",
        "  correct_labels = testData['contains_bug'].to_numpy()\n",
        "  testDataDropLabels = testData.drop('contains_bug', axis=1).to_numpy()\n",
        "  prediction = 0\n",
        "  for i in range(len(testDataDropLabels)):\n",
        "    recall = []\n",
        "    recall.append(recalls[0][-1])\n",
        "    recall.append(recalls[1][-1])\n",
        "    correct_label = int(correct_labels[i])\n",
        "    R[correct_label] = teta * R[correct_label] + int(prediction == correct_label)\n",
        "    N[correct_label] = teta * N[correct_label] + 1\n",
        "    recall_value = round((R[correct_label]/N[correct_label])*100,2)\n",
        "    recalls[correct_label].append(recall_value)\n",
        "    temp[correct_label].append(recall_value)\n",
        "\n",
        "    recalls[1-correct_label].append(recall[1-correct_label])\n",
        "    temp[1-correct_label].append(recall[1-correct_label])\n",
        "    recallNum += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MAscgXPbxHk"
      },
      "outputs": [],
      "source": [
        "#In the code below, we have obtained recall for defect and clean using equation\n",
        "#in sadia paper which is based on fading factor\n",
        "def evaluation(online_bagging_classifier, testData):\n",
        "  global recallNum\n",
        "  teta=0.99\n",
        "  correct_labels = testData['contains_bug'].to_numpy()\n",
        "  testDataDropLabels = testData.drop('contains_bug', axis=1).to_numpy()\n",
        "  predictions = online_bagging_classifier.predict(testDataDropLabels)\n",
        "\n",
        "  for i in range(len(testDataDropLabels)):\n",
        "    recall = []\n",
        "    recall.append(recalls[0][-1])\n",
        "    recall.append(recalls[1][-1])\n",
        "\n",
        "    correct_label = int(correct_labels[i])\n",
        "    R[correct_label] = teta * R[correct_label] + int(predictions[i] == correct_label)\n",
        "    N[correct_label] = teta * N[correct_label] + 1\n",
        "    recall_value = round((R[correct_label]/N[correct_label])*100,2)\n",
        "    recalls[correct_label].append(recall_value)\n",
        "    temp[correct_label].append(recall_value)\n",
        "\n",
        "    recalls[1-correct_label].append(recall[1-correct_label])\n",
        "    temp[1-correct_label].append(recall[1-correct_label])\n",
        "    recallNum += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7TFt_r1yDTj"
      },
      "outputs": [],
      "source": [
        "def data_separation (data):\n",
        "\n",
        "  clean_samples = data.drop(data[data['contains_bug'] == 1].index).drop('contains_bug', axis=1)\n",
        "  defect_samples = data.drop(data[data['contains_bug'] == 0].index).drop('contains_bug', axis=1)\n",
        "\n",
        "  return clean_samples,defect_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVQsIUzLj4XA"
      },
      "outputs": [],
      "source": [
        "def data_normalization(data,status):\n",
        "\n",
        "  if status == 1:\n",
        "    #entropy = entropy/log2(n)\n",
        "    data['entrophy'] = data.apply(lambda row: row['entrophy'] / np.log2(row['nf']) if (row['nf'] != 0 and row['nf'] != 1) else row['entrophy'], axis=1)\n",
        "    data['nuc'] = data.apply(lambda row: row['nuc'] / row['nf'] if row['nf'] != 0 else row['nuc'], axis=1)\n",
        "    data['lt'] = data.apply(lambda row: row['lt'] / row['nf'] if row['nf'] != 0 else row['lt'], axis=1)\n",
        "    data['lt'] = np.abs(data['lt'])\n",
        "    data['churn'] = data.apply(lambda row: (row['la'] + row['ld']) / row['lt'] if row['lt'] != 0 else row['la'] + row['ld'], axis=1)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46RrUYSMxLRm"
      },
      "outputs": [],
      "source": [
        "def remove_correlation_features (data, Remove_extra_correlations_status):\n",
        "\n",
        "  correlated_features = []\n",
        "  #Check the correlation between the columns as 2 to 2\n",
        "  if Remove_extra_correlations_status == 1:\n",
        "    columns = data.columns.tolist()\n",
        "    for col1, col2 in combinations(columns, 2):\n",
        "      correlation, p_value = spearmanr(data[col1], data[col2])\n",
        "      if correlation > 0.8 and p_value < 0.05:\n",
        "        if col1 not in correlated_features and col2 not in correlated_features:\n",
        "          correlated_features.append(col1)\n",
        "  independent_data = data.drop(correlated_features, axis=1)\n",
        "\n",
        "  return independent_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ojHUoVZawdo"
      },
      "outputs": [],
      "source": [
        "def log_transform (data, status):\n",
        "\n",
        "  if status == 1:\n",
        "    data = data.astype(float)\n",
        "    data = np.log2(data[data != 0])\n",
        "    data.fillna(0, inplace=True)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMwDumoPmG2k"
      },
      "outputs": [],
      "source": [
        "def data_scaling (data, status):\n",
        "\n",
        "  if status == 1:\n",
        "    scaler = StandardScaler()\n",
        "    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQKX79mr4_hF"
      },
      "outputs": [],
      "source": [
        "def PCA_decomposition(data, n_com, status):\n",
        "\n",
        "  if status == 1:\n",
        "    pca = PCA(n_components = n_com)\n",
        "    data_transformed = pca.fit_transform(data)\n",
        "    data_transformed = pd.DataFrame(data_transformed)\n",
        "    data_transformed.columns = data_transformed.columns.astype(str)\n",
        "    return data_transformed\n",
        "  else:\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMtXdxc6xZf8"
      },
      "outputs": [],
      "source": [
        "def drop_nf0(data, status):\n",
        "  if status == 1:\n",
        "    #To remove the rows in which the number of changed files is equal to 0. Because it demostrates that no changes have been applied to the commit\n",
        "    data = data[data['nf'] != 0].reset_index(drop=True)\n",
        "  else:\n",
        "    data = data.reset_index(drop=True)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mT2oAv7EaJT"
      },
      "source": [
        "In this part, each line shows a step of data processing. So you can optionally remove each line and evaluate the overall performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSwJeYBN7-1q"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing (data):\n",
        "\n",
        "  data = drop_nf0(data, 0)\n",
        "\n",
        "  #normalizing data except fix feature and label column ('contains_bug')\n",
        "  column_fix = data['fix']\n",
        "  column_contains_bug = data['contains_bug']\n",
        "  #column_classification = data['classification']\n",
        "  data = data.drop('fix', axis=1).drop('contains_bug', axis=1).drop('dnum', axis=1).drop('month', axis=1).drop('year', axis=1).drop('author_date', axis=1)#.drop('classification', axis=1)\n",
        "\n",
        "  data = data_normalization(data,1)\n",
        "  #print('normalized_data: ', data.head(1))\n",
        "\n",
        "  #1) nf, exp yes. etc no\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1).drop('nd', axis=1).drop('ns', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  #2) only nf, etc\n",
        "  #data = data.drop('exp', axis=1).drop('ndev', axis=1).drop('age', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  #3) nf only no etc\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1).drop('nd', axis=1).drop('ns', axis=1).drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "  #data = data.drop('exp', axis=1).drop('ndev', axis=1).drop('age', axis=1)\n",
        "\n",
        "  #4) no nf etc\n",
        "  #data = data.drop('la', axis=1).drop('ld', axis=1)\n",
        "\n",
        "  #data = data.drop('nd', axis=1).drop('ns', axis=1)\n",
        "\n",
        "  #5) no exp etc\n",
        "  #data = data.drop('rexp', axis=1).drop('sexp', axis=1)\n",
        "\n",
        "  data = log_transform (data, 1)\n",
        "  #data = data_scaling(data, 1)\n",
        "  data['fix'] = column_fix\n",
        "  final_data = remove_correlation_features(data, 0)\n",
        "  #finishing preprocessing steps\n",
        "\n",
        "  final_data['contains_bug'] = column_contains_bug\n",
        "\n",
        "  return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srKHFsujIk8H"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log2\")\n",
        "#------------------------------------------------------------------------------- Settings\n",
        "wp = True\n",
        "if wp: pklFile_rec = 'oob_recalls_wp.pkl'\n",
        "else: pklFile_rec = 'oob_recalls_dfs.pkl'\n",
        "interpret_defectPredict = True\n",
        "#------------------------------------------------------------------------------- Import datasets\n",
        "input_dir = \"drive/MyDrive/part2/data/sadia_csv/\"\n",
        "output_dir = 'drive/MyDrive/part2/data/code_outputs/'\n",
        "dfs_path = 'drive/MyDrive/part2/data/dfs' + '.pkl'\n",
        "files = os.listdir(input_dir)\n",
        "for filename in files:\n",
        "  target_fileName = filename[:-4]\n",
        "  intpResult_folder = output_dir + target_fileName + '/shap_result/'\n",
        "  if not os.path.exists(intpResult_folder + 'shap_gmm.csv'):\n",
        "    dfTest_path = output_dir + target_fileName + '/df_test.pkl'\n",
        "    if os.path.isfile(dfTest_path):\n",
        "      with open(dfTest_path, 'rb') as file:\n",
        "          df_test = pickle.load(file)\n",
        "          print('There is the df_test file for ' + target_fileName + ' project.')\n",
        "    else: df_test = separating_daily(output_dir, input_dir, target_fileName, dfTest_path)\n",
        "\n",
        "    if wp:\n",
        "      dfs = df_test\n",
        "    else:\n",
        "      if os.path.isfile(dfs_path):\n",
        "        # Load the array dfs from the saved file\n",
        "        with open(dfs_path , 'rb') as file:\n",
        "          dfs = pickle.load(file)\n",
        "          print('There is the dfs' + ' file.')\n",
        "      else: dfs = merging_datasets(output_dir, input_dir, dfs_path)\n",
        "\n",
        "  #------------------------------------------------------------------------------- Main code\n",
        "    start_time = time.time()\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log2\")\n",
        "    all_start_time = time.time()\n",
        "  #------------------------------------------------------------------------------- Adjust the settings\n",
        "    UpMflag = True # Update model flag\n",
        "    trainData_len = 1\n",
        "    Initial_size = 1 # The minimum number of samples required to build the model\n",
        "    vl = 90 # verification latency\n",
        "    startNum = 0\n",
        "    endNum = 5\n",
        "    rowNum = 0\n",
        "    defect_accsMean = 0\n",
        "    clean_accsMean = 0\n",
        "    flag = False\n",
        "    initData = pd.DataFrame()\n",
        "    testNum = 0\n",
        "    trainNum = 0\n",
        "  #------------------------------------------------------------------------------- Global variables\n",
        "    global imRate\n",
        "    imRate = [1, 1]\n",
        "    global first_imRate\n",
        "    first_imRate = [True, True]\n",
        "    global num_init_sam_rate\n",
        "    num_init_sam_rate = [0, 0]\n",
        "    #If the new sample is a member of class C, the recall of the other class remains the same as the previous recall\n",
        "    global recalls\n",
        "    recalls = [[0],[0]]\n",
        "    global recallNum\n",
        "    recallNum = 0\n",
        "    global temp\n",
        "    temp = [[0],[0]]\n",
        "    global N\n",
        "    N = [0, 0]\n",
        "    global R\n",
        "    R = [0, 0]\n",
        "    global iN\n",
        "    iN = [1, 1]\n",
        "    global iR\n",
        "    iR = [1, 1]\n",
        "    def updatemodel (UpMflag, trainData, i):\n",
        "      if UpMflag: return pd.concat([trainData, dfs[i]], ignore_index=True)\n",
        "      else: return trainData\n",
        "  #------------------------------------------------------------------------------ Creating training DS\n",
        "\n",
        "    while True:\n",
        "      #this shows, wether there is any elements in this section (for larger speed)\n",
        "      if len(pd.concat(dfs[startNum:endNum], ignore_index=True)) > 0:\n",
        "        for rowNum in range(startNum,endNum):\n",
        "          initData = pd.concat([initData, dfs[rowNum]], ignore_index=True)\n",
        "          if len(initData) > Initial_size:\n",
        "            preprocessed_data = data_preprocessing(initData)\n",
        "            normalized_clean_samples, normalized_defect_samples = data_separation(preprocessed_data)\n",
        "            if len(normalized_defect_samples) > 1 and len(normalized_clean_samples) > 1:\n",
        "              trainNum = len(preprocessed_data)\n",
        "              print('num clean: ', len(normalized_clean_samples))\n",
        "              print('num defect: ', len(normalized_defect_samples))\n",
        "              flag = True\n",
        "              break\n",
        "      if flag:\n",
        "        break\n",
        "\n",
        "      startNum = endNum\n",
        "      endNum += 5\n",
        "    print(len(initData), ' - ', rowNum, ' - ', trainNum)\n",
        "\n",
        "    #----------------------------------------------------------------------------- Initial modeling\n",
        "    testCount = rowNum+vl\n",
        "    trainData = pd.DataFrame()\n",
        "    testData = pd.concat(df_test[:testCount+1], ignore_index=True)\n",
        "    print('len testData:',len(testData))\n",
        "    if len(testData) > 1:\n",
        "      gmeans_temp = []\n",
        "      print('len testData:',len(testData))\n",
        "      testNum = len(testData)\n",
        "      initial_evaluation(testData)\n",
        "      recalls[0].pop(0)\n",
        "      recalls[1].pop(0)\n",
        "      if len(recalls[1])>0:    defect_accsMean = sum(recalls[1]) / len(recalls[1])\n",
        "      if len(recalls[0])>0:  clean_accsMean = sum(recalls[0]) / len(recalls[0])\n",
        "      print(\"idefect_accsMean: \", defect_accsMean, \" - defect_accs: \", recalls[1])\n",
        "      print(\"iclean_accsMean:\", clean_accsMean, \" - clean_accs:\", recalls[0])\n",
        "      if N[0]>0 and N[1]>0:\n",
        "        for j in range(len(temp[0])):\n",
        "          gmeans_temp.append(math.sqrt(temp[0][j] * temp[1][j]))\n",
        "      print('N: ', N)\n",
        "\n",
        "\n",
        "    init_recalls0_len = len(recalls[0])\n",
        "    init_recalls1_len = len(recalls[1])\n",
        "    temp[0] = []\n",
        "    temp[1] = []\n",
        "\n",
        "    print('init testCount: ', testCount)\n",
        "    print('init train num: ', trainNum)\n",
        "    print('init test num: ', testNum)\n",
        "    print('init recall num: ', recallNum)\n",
        "  #----------------------------------------------------------------------------- Training model\n",
        "    online_bagging_classifier = create_initial_model(preprocessed_data)\n",
        "    if interpret_defectPredict: explainer = shap.KernelExplainer(online_bagging_classifier.predict, normalized_defect_samples)\n",
        "    else: explainer = shap.KernelExplainer(online_bagging_classifier.predict, normalized_clean_samples)\n",
        "\n",
        "    times = []\n",
        "    trCln = 0\n",
        "    defect_ratio = 0\n",
        "    predictions = []\n",
        "    clnNum = 0\n",
        "    defNum = 0\n",
        "  #----------------------------------------------------------------------------- Interpretation variables\n",
        "    best_feature = 'best_feature'\n",
        "    last_NS = pd.DataFrame() #last normalized_samples\n",
        "    counterCD = 0\n",
        "    cd_points = []\n",
        "    df_shap = pd.DataFrame(columns=normalized_clean_samples.columns)\n",
        "    ind_shap = 0\n",
        "    #-----------------------------------------------------------------------------\n",
        "    for i in range(rowNum+1, len(df_test) - 1 - vl):\n",
        "      if i%100 == 0:\n",
        "        all_time = int(time.time()- start_time)\n",
        "        gmeans_temp = []\n",
        "        print(f\"Execution time: {all_time} seconds\")\n",
        "        if len(temp[0])>0: times.append(all_time)\n",
        "        start_time = time.time()\n",
        "        print(i, ' ******************** - ', target_fileName, ': ', len(temp[0]), ' - ', len(temp[1]) )\n",
        "        if len(temp[1])>0:    defect_accsMean = sum(temp[1]) / len(temp[1])\n",
        "        if len(temp[0])>0:  clean_accsMean = sum(temp[0]) / len(temp[0])\n",
        "        print(\"defect_accsMean: \", defect_accsMean, \" - defect_accs: \", temp[1])\n",
        "        print(\"clean_accsMean:\", clean_accsMean, \" - clean_accs:\", temp[0])\n",
        "        temp[0] = []\n",
        "        temp[1] = []\n",
        "      trainData = updatemodel(UpMflag, trainData, i)\n",
        "      #---------------------------------------------------------------------------- Adapting model\n",
        "      if len(trainData) > trainData_len:\n",
        "        preprocessed_data = data_preprocessing(trainData)\n",
        "        normalized_clean_samples, normalized_defect_samples = data_separation(preprocessed_data)\n",
        "        if interpret_defectPredict: last_NS = pd.concat([last_NDS, normalized_defect_samples], ignore_index=True)\n",
        "        else: last_NS = pd.concat([last_NCS, normalized_clean_samples], ignore_index=True)\n",
        "        online_bagging_classifier = update_model(online_bagging_classifier, preprocessed_data)\n",
        "        trainData = pd.DataFrame()\n",
        "        trainNum += len(preprocessed_data)\n",
        "      #--------------------------------------------------------------------------- Testing model\n",
        "      testCount += 1\n",
        "      testData = df_test[testCount]\n",
        "      if len(testData) > 0:\n",
        "        preprocessed_data = data_preprocessing(testData)\n",
        "        evaluation(online_bagging_classifier, preprocessed_data)\n",
        "        testData = pd.DataFrame()\n",
        "        testNum += len(preprocessed_data)\n",
        "  #------------------------------------------------------------------------------- Interpretation\n",
        "\n",
        "        X_test = preprocessed_data.drop('contains_bug', axis=1)\n",
        "        shap_values = explainer.shap_values(X_test)\n",
        "        feature_importance = np.round(np.mean(np.abs(shap_values), axis=0),2)\n",
        "        feature_indices = np.argsort(feature_importance)[::-1]\n",
        "        num_top_features = 2\n",
        "        if best_feature == 'best_feature':\n",
        "          best_feature = X_test.columns[feature_indices[0]]\n",
        "        for j in feature_indices[:num_top_features]:\n",
        "          print(f\"Feature {X_test.columns[j]}: Importance = {feature_importance[j]:.3f}\")\n",
        "        if not best_feature == X_test.columns[feature_indices[0]] and len(last_NS)>0:\n",
        "          counterCD += 1\n",
        "          explainer = shap.KernelExplainer(online_bagging_classifier.predict, last_NS)\n",
        "          print('************************* Explainer is updated')\n",
        "          print('len last_NS: ', len(last_NS))\n",
        "          cd_points.append(testNum)\n",
        "          best_feature = X_test.columns[feature_indices[0]]\n",
        "          last_NS = pd.DataFrame()\n",
        "        new_row_df = pd.DataFrame(shap_values, columns=df_shap.columns)\n",
        "        df_shap = pd.concat([df_shap, new_row_df], ignore_index=True)\n",
        "  #------------------------------------------------------------------------------- Saving interpretation result files\n",
        "    print('counterCD for ', target_fileName, ': ', counterCD)\n",
        "    csvFolder_name = output_dir + target_fileName + '/shap_result/'\n",
        "    if not os.path.exists(csvFolder_name):\n",
        "        os.makedirs(csvFolder_name)\n",
        "        print(f\"Folder '{csvFolder_name}' created successfully.\")\n",
        "    df_shap.to_csv(csvFolder_name + 'shap_oob.csv', index=False)\n",
        "    with open(csvFolder_name + 'cdPoints_oob', 'wb') as file:\n",
        "      pickle.dump(cd_points, file)\n",
        "  #-------------------------------------------------------------------------------\n",
        "    recalls[0].pop(0)\n",
        "    recalls[1].pop(0)\n",
        "    defect_accsMean = sum(recalls[1]) / len(recalls[1])\n",
        "    clean_accsMean = sum(recalls[0]) / len(recalls[0])\n",
        "    print(\"defect_accsMean: \", defect_accsMean, \" - defect_accs: \", recalls[1])\n",
        "    print(\"clean_accsMean:\", clean_accsMean, \" - clean_accs:\", recalls[0])\n",
        "    gmeans = []\n",
        "    for i in range(len(recalls[0])):\n",
        "      gmeans.append(math.sqrt(recalls[0][i] * recalls[1][i]))\n",
        "\n",
        "    pkFolder_name = output_dir + target_fileName + '/recallFiles/'\n",
        "    if not os.path.exists(pkFolder_name):\n",
        "        os.makedirs(pkFolder_name)\n",
        "        print(f\"Folder '{pkFolder_name}' created successfully.\")\n",
        "    with open(pkFolder_name + pklFile_rec, 'wb') as file:\n",
        "        pickle.dump(recalls, file)\n",
        "\n",
        "    gmeans_avg = sum(gmeans) / len(gmeans)\n",
        "    time_avg = sum(times)/len(times)\n",
        "    print(\"gmeans_avg:\", gmeans_avg)\n",
        "    print('target_fileName: ', target_fileName)\n",
        "    print('time_avg: ', math.floor(time_avg + 0.5), ' second')\n",
        "    print('remained testData: ', len(testData))\n",
        "    print('init_recalls0_len: ', init_recalls0_len)\n",
        "    print('init_recalls1_len: ', init_recalls1_len)\n",
        "    print('recalls0_len: ', len(recalls[0]))\n",
        "    print('recalls1_len: ', len(recalls[1]))\n",
        "    print('end train num: ', trainNum)\n",
        "    print('end test num: ', testNum)\n",
        "    print('end recall num: ', recallNum)\n",
        "    print('All Time = ', math.floor(int(time.time() - all_start_time)/60 + 0.5), ' minutes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63bE34gt-DWZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(recalls[1], 'ro', linestyle='dotted')\n",
        "#plt.plot(accs, 'bo', linestyle='dotted')\n",
        "plt.plot(recalls[0], 'go', linestyle='dotted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Line Diagram for Array')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQa7op9u4DGN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "arr = []\n",
        "arr.append(recalls[1])\n",
        "arr.append(recalls[0])\n",
        "arr.append(gmeans)\n",
        "plt.boxplot(arr)\n",
        "#plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJ6LcFczy7h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}